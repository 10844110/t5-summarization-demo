{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 215337,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006965825659315398,
      "grad_norm": 3.155489444732666,
      "learning_rate": 4.9883902905678084e-05,
      "loss": 1.9963,
      "step": 500
    },
    {
      "epoch": 0.013931651318630797,
      "grad_norm": 2.5873303413391113,
      "learning_rate": 4.976780581135616e-05,
      "loss": 1.907,
      "step": 1000
    },
    {
      "epoch": 0.020897476977946194,
      "grad_norm": 2.237546682357788,
      "learning_rate": 4.965170871703423e-05,
      "loss": 1.885,
      "step": 1500
    },
    {
      "epoch": 0.027863302637261594,
      "grad_norm": 2.2880873680114746,
      "learning_rate": 4.953561162271231e-05,
      "loss": 1.9094,
      "step": 2000
    },
    {
      "epoch": 0.03482912829657699,
      "grad_norm": 2.4343106746673584,
      "learning_rate": 4.941951452839039e-05,
      "loss": 1.8817,
      "step": 2500
    },
    {
      "epoch": 0.04179495395589239,
      "grad_norm": 2.691567897796631,
      "learning_rate": 4.9303417434068464e-05,
      "loss": 1.9018,
      "step": 3000
    },
    {
      "epoch": 0.04876077961520779,
      "grad_norm": 2.318382740020752,
      "learning_rate": 4.918732033974654e-05,
      "loss": 1.9088,
      "step": 3500
    },
    {
      "epoch": 0.05572660527452319,
      "grad_norm": 2.165414333343506,
      "learning_rate": 4.907122324542461e-05,
      "loss": 1.9017,
      "step": 4000
    },
    {
      "epoch": 0.06269243093383858,
      "grad_norm": 2.429349899291992,
      "learning_rate": 4.8955126151102695e-05,
      "loss": 1.9169,
      "step": 4500
    },
    {
      "epoch": 0.06965825659315399,
      "grad_norm": 2.744922161102295,
      "learning_rate": 4.883902905678077e-05,
      "loss": 1.8711,
      "step": 5000
    },
    {
      "epoch": 0.07662408225246939,
      "grad_norm": 2.495335817337036,
      "learning_rate": 4.8722931962458844e-05,
      "loss": 1.873,
      "step": 5500
    },
    {
      "epoch": 0.08358990791178478,
      "grad_norm": 2.3005359172821045,
      "learning_rate": 4.8606834868136925e-05,
      "loss": 1.8507,
      "step": 6000
    },
    {
      "epoch": 0.09055573357110018,
      "grad_norm": 2.424417495727539,
      "learning_rate": 4.8490737773815e-05,
      "loss": 1.8611,
      "step": 6500
    },
    {
      "epoch": 0.09752155923041558,
      "grad_norm": 2.5702176094055176,
      "learning_rate": 4.8374640679493075e-05,
      "loss": 1.8801,
      "step": 7000
    },
    {
      "epoch": 0.10448738488973099,
      "grad_norm": 2.255692481994629,
      "learning_rate": 4.825854358517115e-05,
      "loss": 1.8609,
      "step": 7500
    },
    {
      "epoch": 0.11145321054904637,
      "grad_norm": 2.2190864086151123,
      "learning_rate": 4.814244649084923e-05,
      "loss": 1.9013,
      "step": 8000
    },
    {
      "epoch": 0.11841903620836178,
      "grad_norm": 2.3233108520507812,
      "learning_rate": 4.8026349396527306e-05,
      "loss": 1.8774,
      "step": 8500
    },
    {
      "epoch": 0.12538486186767717,
      "grad_norm": 2.748290538787842,
      "learning_rate": 4.791025230220538e-05,
      "loss": 1.8929,
      "step": 9000
    },
    {
      "epoch": 0.13235068752699258,
      "grad_norm": 2.465764045715332,
      "learning_rate": 4.779415520788346e-05,
      "loss": 1.8876,
      "step": 9500
    },
    {
      "epoch": 0.13931651318630797,
      "grad_norm": 2.2470476627349854,
      "learning_rate": 4.7678058113561536e-05,
      "loss": 1.8639,
      "step": 10000
    },
    {
      "epoch": 0.14628233884562336,
      "grad_norm": 2.3704261779785156,
      "learning_rate": 4.756196101923962e-05,
      "loss": 1.8812,
      "step": 10500
    },
    {
      "epoch": 0.15324816450493878,
      "grad_norm": 2.8537204265594482,
      "learning_rate": 4.7445863924917686e-05,
      "loss": 1.8658,
      "step": 11000
    },
    {
      "epoch": 0.16021399016425417,
      "grad_norm": 2.738137722015381,
      "learning_rate": 4.732976683059577e-05,
      "loss": 1.8534,
      "step": 11500
    },
    {
      "epoch": 0.16717981582356956,
      "grad_norm": 3.0686872005462646,
      "learning_rate": 4.721366973627384e-05,
      "loss": 1.8942,
      "step": 12000
    },
    {
      "epoch": 0.17414564148288497,
      "grad_norm": 3.0573699474334717,
      "learning_rate": 4.709757264195192e-05,
      "loss": 1.8533,
      "step": 12500
    },
    {
      "epoch": 0.18111146714220036,
      "grad_norm": 2.447821617126465,
      "learning_rate": 4.698147554762999e-05,
      "loss": 1.8569,
      "step": 13000
    },
    {
      "epoch": 0.18807729280151578,
      "grad_norm": 2.497950553894043,
      "learning_rate": 4.686537845330807e-05,
      "loss": 1.8596,
      "step": 13500
    },
    {
      "epoch": 0.19504311846083117,
      "grad_norm": 2.2902324199676514,
      "learning_rate": 4.6749281358986154e-05,
      "loss": 1.8668,
      "step": 14000
    },
    {
      "epoch": 0.20200894412014656,
      "grad_norm": 2.3872475624084473,
      "learning_rate": 4.663318426466423e-05,
      "loss": 1.8522,
      "step": 14500
    },
    {
      "epoch": 0.20897476977946197,
      "grad_norm": 2.632460832595825,
      "learning_rate": 4.65170871703423e-05,
      "loss": 1.8874,
      "step": 15000
    },
    {
      "epoch": 0.21594059543877736,
      "grad_norm": 2.6769042015075684,
      "learning_rate": 4.640099007602038e-05,
      "loss": 1.8636,
      "step": 15500
    },
    {
      "epoch": 0.22290642109809275,
      "grad_norm": 2.5163259506225586,
      "learning_rate": 4.628489298169846e-05,
      "loss": 1.8726,
      "step": 16000
    },
    {
      "epoch": 0.22987224675740817,
      "grad_norm": 2.2305939197540283,
      "learning_rate": 4.6168795887376534e-05,
      "loss": 1.8735,
      "step": 16500
    },
    {
      "epoch": 0.23683807241672356,
      "grad_norm": 3.0273497104644775,
      "learning_rate": 4.605269879305461e-05,
      "loss": 1.8531,
      "step": 17000
    },
    {
      "epoch": 0.24380389807603894,
      "grad_norm": 2.031238555908203,
      "learning_rate": 4.5936601698732683e-05,
      "loss": 1.8471,
      "step": 17500
    },
    {
      "epoch": 0.25076972373535433,
      "grad_norm": 2.2419064044952393,
      "learning_rate": 4.5820504604410765e-05,
      "loss": 1.8559,
      "step": 18000
    },
    {
      "epoch": 0.25773554939466975,
      "grad_norm": 2.6566429138183594,
      "learning_rate": 4.570440751008884e-05,
      "loss": 1.855,
      "step": 18500
    },
    {
      "epoch": 0.26470137505398517,
      "grad_norm": 2.164769411087036,
      "learning_rate": 4.5588310415766914e-05,
      "loss": 1.8523,
      "step": 19000
    },
    {
      "epoch": 0.2716672007133005,
      "grad_norm": 2.400770664215088,
      "learning_rate": 4.5472213321444996e-05,
      "loss": 1.8797,
      "step": 19500
    },
    {
      "epoch": 0.27863302637261594,
      "grad_norm": 1.7558810710906982,
      "learning_rate": 4.535611622712307e-05,
      "loss": 1.8631,
      "step": 20000
    },
    {
      "epoch": 0.28559885203193136,
      "grad_norm": 2.608041524887085,
      "learning_rate": 4.5240019132801145e-05,
      "loss": 1.8635,
      "step": 20500
    },
    {
      "epoch": 0.2925646776912467,
      "grad_norm": 2.517634868621826,
      "learning_rate": 4.512392203847922e-05,
      "loss": 1.8551,
      "step": 21000
    },
    {
      "epoch": 0.29953050335056214,
      "grad_norm": 2.8861422538757324,
      "learning_rate": 4.50078249441573e-05,
      "loss": 1.8488,
      "step": 21500
    },
    {
      "epoch": 0.30649632900987756,
      "grad_norm": 2.80824613571167,
      "learning_rate": 4.4891727849835376e-05,
      "loss": 1.8563,
      "step": 22000
    },
    {
      "epoch": 0.3134621546691929,
      "grad_norm": 2.4736411571502686,
      "learning_rate": 4.477563075551345e-05,
      "loss": 1.8441,
      "step": 22500
    },
    {
      "epoch": 0.32042798032850833,
      "grad_norm": 2.8290841579437256,
      "learning_rate": 4.4659533661191525e-05,
      "loss": 1.8674,
      "step": 23000
    },
    {
      "epoch": 0.32739380598782375,
      "grad_norm": 2.0041098594665527,
      "learning_rate": 4.4543436566869607e-05,
      "loss": 1.869,
      "step": 23500
    },
    {
      "epoch": 0.3343596316471391,
      "grad_norm": 2.8183419704437256,
      "learning_rate": 4.442733947254769e-05,
      "loss": 1.8453,
      "step": 24000
    },
    {
      "epoch": 0.3413254573064545,
      "grad_norm": 2.492727756500244,
      "learning_rate": 4.4311242378225756e-05,
      "loss": 1.8589,
      "step": 24500
    },
    {
      "epoch": 0.34829128296576994,
      "grad_norm": 2.338071823120117,
      "learning_rate": 4.419514528390384e-05,
      "loss": 1.8613,
      "step": 25000
    },
    {
      "epoch": 0.3552571086250853,
      "grad_norm": 2.4829013347625732,
      "learning_rate": 4.407904818958191e-05,
      "loss": 1.8615,
      "step": 25500
    },
    {
      "epoch": 0.3622229342844007,
      "grad_norm": 2.1780879497528076,
      "learning_rate": 4.3962951095259993e-05,
      "loss": 1.8777,
      "step": 26000
    },
    {
      "epoch": 0.36918875994371614,
      "grad_norm": 2.8081891536712646,
      "learning_rate": 4.384685400093806e-05,
      "loss": 1.8398,
      "step": 26500
    },
    {
      "epoch": 0.37615458560303155,
      "grad_norm": 2.522352933883667,
      "learning_rate": 4.373075690661614e-05,
      "loss": 1.8525,
      "step": 27000
    },
    {
      "epoch": 0.3831204112623469,
      "grad_norm": 2.9891324043273926,
      "learning_rate": 4.361465981229422e-05,
      "loss": 1.865,
      "step": 27500
    },
    {
      "epoch": 0.39008623692166233,
      "grad_norm": 2.7652931213378906,
      "learning_rate": 4.34985627179723e-05,
      "loss": 1.8681,
      "step": 28000
    },
    {
      "epoch": 0.39705206258097775,
      "grad_norm": 2.3690905570983887,
      "learning_rate": 4.3382465623650374e-05,
      "loss": 1.8397,
      "step": 28500
    },
    {
      "epoch": 0.4040178882402931,
      "grad_norm": 2.7173612117767334,
      "learning_rate": 4.326636852932845e-05,
      "loss": 1.8491,
      "step": 29000
    },
    {
      "epoch": 0.4109837138996085,
      "grad_norm": 2.066134452819824,
      "learning_rate": 4.315027143500653e-05,
      "loss": 1.8288,
      "step": 29500
    },
    {
      "epoch": 0.41794953955892394,
      "grad_norm": 2.8027124404907227,
      "learning_rate": 4.3034174340684604e-05,
      "loss": 1.8675,
      "step": 30000
    },
    {
      "epoch": 0.4249153652182393,
      "grad_norm": 2.7825443744659424,
      "learning_rate": 4.291807724636268e-05,
      "loss": 1.8481,
      "step": 30500
    },
    {
      "epoch": 0.4318811908775547,
      "grad_norm": 2.602241277694702,
      "learning_rate": 4.2801980152040754e-05,
      "loss": 1.8617,
      "step": 31000
    },
    {
      "epoch": 0.43884701653687014,
      "grad_norm": 2.904597282409668,
      "learning_rate": 4.2685883057718835e-05,
      "loss": 1.8351,
      "step": 31500
    },
    {
      "epoch": 0.4458128421961855,
      "grad_norm": 3.496182918548584,
      "learning_rate": 4.256978596339691e-05,
      "loss": 1.8543,
      "step": 32000
    },
    {
      "epoch": 0.4527786678555009,
      "grad_norm": 2.294245481491089,
      "learning_rate": 4.2453688869074984e-05,
      "loss": 1.8257,
      "step": 32500
    },
    {
      "epoch": 0.45974449351481633,
      "grad_norm": 2.3762826919555664,
      "learning_rate": 4.2337591774753066e-05,
      "loss": 1.8303,
      "step": 33000
    },
    {
      "epoch": 0.4667103191741317,
      "grad_norm": 2.327807903289795,
      "learning_rate": 4.222149468043114e-05,
      "loss": 1.8411,
      "step": 33500
    },
    {
      "epoch": 0.4736761448334471,
      "grad_norm": 2.4564146995544434,
      "learning_rate": 4.210539758610922e-05,
      "loss": 1.8385,
      "step": 34000
    },
    {
      "epoch": 0.4806419704927625,
      "grad_norm": 3.019453525543213,
      "learning_rate": 4.198930049178729e-05,
      "loss": 1.8301,
      "step": 34500
    },
    {
      "epoch": 0.4876077961520779,
      "grad_norm": 2.4043262004852295,
      "learning_rate": 4.187320339746537e-05,
      "loss": 1.8411,
      "step": 35000
    },
    {
      "epoch": 0.4945736218113933,
      "grad_norm": 2.705796718597412,
      "learning_rate": 4.1757106303143446e-05,
      "loss": 1.846,
      "step": 35500
    },
    {
      "epoch": 0.5015394474707087,
      "grad_norm": 2.4572956562042236,
      "learning_rate": 4.164100920882153e-05,
      "loss": 1.8582,
      "step": 36000
    },
    {
      "epoch": 0.5085052731300241,
      "grad_norm": 2.5618956089019775,
      "learning_rate": 4.1524912114499595e-05,
      "loss": 1.8544,
      "step": 36500
    },
    {
      "epoch": 0.5154710987893395,
      "grad_norm": 2.1638076305389404,
      "learning_rate": 4.140881502017768e-05,
      "loss": 1.8548,
      "step": 37000
    },
    {
      "epoch": 0.5224369244486549,
      "grad_norm": 2.108018398284912,
      "learning_rate": 4.129271792585576e-05,
      "loss": 1.821,
      "step": 37500
    },
    {
      "epoch": 0.5294027501079703,
      "grad_norm": 1.9704960584640503,
      "learning_rate": 4.117662083153383e-05,
      "loss": 1.8247,
      "step": 38000
    },
    {
      "epoch": 0.5363685757672857,
      "grad_norm": 2.4771080017089844,
      "learning_rate": 4.106052373721191e-05,
      "loss": 1.8551,
      "step": 38500
    },
    {
      "epoch": 0.543334401426601,
      "grad_norm": 2.5207221508026123,
      "learning_rate": 4.094442664288998e-05,
      "loss": 1.8397,
      "step": 39000
    },
    {
      "epoch": 0.5503002270859165,
      "grad_norm": 2.259183168411255,
      "learning_rate": 4.0828329548568064e-05,
      "loss": 1.8457,
      "step": 39500
    },
    {
      "epoch": 0.5572660527452319,
      "grad_norm": 2.478835344314575,
      "learning_rate": 4.071223245424614e-05,
      "loss": 1.8533,
      "step": 40000
    },
    {
      "epoch": 0.5642318784045472,
      "grad_norm": 2.284909963607788,
      "learning_rate": 4.059613535992421e-05,
      "loss": 1.8266,
      "step": 40500
    },
    {
      "epoch": 0.5711977040638627,
      "grad_norm": 2.323348045349121,
      "learning_rate": 4.048003826560229e-05,
      "loss": 1.8485,
      "step": 41000
    },
    {
      "epoch": 0.5781635297231781,
      "grad_norm": 2.308819532394409,
      "learning_rate": 4.036394117128037e-05,
      "loss": 1.8454,
      "step": 41500
    },
    {
      "epoch": 0.5851293553824934,
      "grad_norm": 2.570941686630249,
      "learning_rate": 4.0247844076958444e-05,
      "loss": 1.8288,
      "step": 42000
    },
    {
      "epoch": 0.5920951810418089,
      "grad_norm": 2.3108861446380615,
      "learning_rate": 4.013174698263652e-05,
      "loss": 1.8244,
      "step": 42500
    },
    {
      "epoch": 0.5990610067011243,
      "grad_norm": 2.5496585369110107,
      "learning_rate": 4.00156498883146e-05,
      "loss": 1.85,
      "step": 43000
    },
    {
      "epoch": 0.6060268323604396,
      "grad_norm": 2.298250675201416,
      "learning_rate": 3.9899552793992674e-05,
      "loss": 1.8272,
      "step": 43500
    },
    {
      "epoch": 0.6129926580197551,
      "grad_norm": 2.065753698348999,
      "learning_rate": 3.978345569967075e-05,
      "loss": 1.8248,
      "step": 44000
    },
    {
      "epoch": 0.6199584836790705,
      "grad_norm": 2.788381338119507,
      "learning_rate": 3.9667358605348824e-05,
      "loss": 1.8108,
      "step": 44500
    },
    {
      "epoch": 0.6269243093383858,
      "grad_norm": 2.363224744796753,
      "learning_rate": 3.9551261511026905e-05,
      "loss": 1.8362,
      "step": 45000
    },
    {
      "epoch": 0.6338901349977013,
      "grad_norm": 2.5474467277526855,
      "learning_rate": 3.943516441670498e-05,
      "loss": 1.851,
      "step": 45500
    },
    {
      "epoch": 0.6408559606570167,
      "grad_norm": 2.2281577587127686,
      "learning_rate": 3.9319067322383055e-05,
      "loss": 1.8091,
      "step": 46000
    },
    {
      "epoch": 0.647821786316332,
      "grad_norm": 2.9146859645843506,
      "learning_rate": 3.9202970228061136e-05,
      "loss": 1.835,
      "step": 46500
    },
    {
      "epoch": 0.6547876119756475,
      "grad_norm": 2.2931342124938965,
      "learning_rate": 3.908687313373921e-05,
      "loss": 1.8383,
      "step": 47000
    },
    {
      "epoch": 0.6617534376349629,
      "grad_norm": 2.4295427799224854,
      "learning_rate": 3.897077603941729e-05,
      "loss": 1.8477,
      "step": 47500
    },
    {
      "epoch": 0.6687192632942782,
      "grad_norm": 2.2442116737365723,
      "learning_rate": 3.885467894509536e-05,
      "loss": 1.8184,
      "step": 48000
    },
    {
      "epoch": 0.6756850889535937,
      "grad_norm": 2.654118537902832,
      "learning_rate": 3.873858185077344e-05,
      "loss": 1.8258,
      "step": 48500
    },
    {
      "epoch": 0.682650914612909,
      "grad_norm": 1.8216413259506226,
      "learning_rate": 3.8622484756451516e-05,
      "loss": 1.8276,
      "step": 49000
    },
    {
      "epoch": 0.6896167402722244,
      "grad_norm": 2.3829407691955566,
      "learning_rate": 3.85063876621296e-05,
      "loss": 1.8296,
      "step": 49500
    },
    {
      "epoch": 0.6965825659315399,
      "grad_norm": 2.6412160396575928,
      "learning_rate": 3.8390290567807665e-05,
      "loss": 1.8437,
      "step": 50000
    },
    {
      "epoch": 0.7035483915908552,
      "grad_norm": 2.7381234169006348,
      "learning_rate": 3.827419347348575e-05,
      "loss": 1.8245,
      "step": 50500
    },
    {
      "epoch": 0.7105142172501706,
      "grad_norm": 2.479600429534912,
      "learning_rate": 3.815809637916383e-05,
      "loss": 1.8376,
      "step": 51000
    },
    {
      "epoch": 0.7174800429094861,
      "grad_norm": 2.4459335803985596,
      "learning_rate": 3.80419992848419e-05,
      "loss": 1.8379,
      "step": 51500
    },
    {
      "epoch": 0.7244458685688014,
      "grad_norm": 1.9636120796203613,
      "learning_rate": 3.792590219051998e-05,
      "loss": 1.8299,
      "step": 52000
    },
    {
      "epoch": 0.7314116942281168,
      "grad_norm": 2.72335147857666,
      "learning_rate": 3.780980509619805e-05,
      "loss": 1.8636,
      "step": 52500
    },
    {
      "epoch": 0.7383775198874323,
      "grad_norm": 2.59999418258667,
      "learning_rate": 3.7693708001876134e-05,
      "loss": 1.8489,
      "step": 53000
    },
    {
      "epoch": 0.7453433455467476,
      "grad_norm": 3.3026862144470215,
      "learning_rate": 3.757761090755421e-05,
      "loss": 1.8147,
      "step": 53500
    },
    {
      "epoch": 0.7523091712060631,
      "grad_norm": 2.324056625366211,
      "learning_rate": 3.746151381323228e-05,
      "loss": 1.8195,
      "step": 54000
    },
    {
      "epoch": 0.7592749968653785,
      "grad_norm": 2.831697463989258,
      "learning_rate": 3.734541671891036e-05,
      "loss": 1.8104,
      "step": 54500
    },
    {
      "epoch": 0.7662408225246938,
      "grad_norm": 2.401489496231079,
      "learning_rate": 3.722931962458844e-05,
      "loss": 1.845,
      "step": 55000
    },
    {
      "epoch": 0.7732066481840093,
      "grad_norm": 2.1180336475372314,
      "learning_rate": 3.7113222530266514e-05,
      "loss": 1.8116,
      "step": 55500
    },
    {
      "epoch": 0.7801724738433247,
      "grad_norm": 2.2862353324890137,
      "learning_rate": 3.699712543594459e-05,
      "loss": 1.8242,
      "step": 56000
    },
    {
      "epoch": 0.78713829950264,
      "grad_norm": 2.788621664047241,
      "learning_rate": 3.688102834162267e-05,
      "loss": 1.8275,
      "step": 56500
    },
    {
      "epoch": 0.7941041251619555,
      "grad_norm": 2.1678059101104736,
      "learning_rate": 3.6764931247300745e-05,
      "loss": 1.8548,
      "step": 57000
    },
    {
      "epoch": 0.8010699508212709,
      "grad_norm": 2.233086109161377,
      "learning_rate": 3.664883415297882e-05,
      "loss": 1.8145,
      "step": 57500
    },
    {
      "epoch": 0.8080357764805862,
      "grad_norm": 2.2300753593444824,
      "learning_rate": 3.6532737058656894e-05,
      "loss": 1.8259,
      "step": 58000
    },
    {
      "epoch": 0.8150016021399017,
      "grad_norm": 2.084829807281494,
      "learning_rate": 3.6416639964334975e-05,
      "loss": 1.8368,
      "step": 58500
    },
    {
      "epoch": 0.821967427799217,
      "grad_norm": 2.1640849113464355,
      "learning_rate": 3.630054287001305e-05,
      "loss": 1.8289,
      "step": 59000
    },
    {
      "epoch": 0.8289332534585324,
      "grad_norm": 1.9943181276321411,
      "learning_rate": 3.618444577569113e-05,
      "loss": 1.8142,
      "step": 59500
    },
    {
      "epoch": 0.8358990791178479,
      "grad_norm": 2.6706619262695312,
      "learning_rate": 3.60683486813692e-05,
      "loss": 1.8333,
      "step": 60000
    },
    {
      "epoch": 0.8428649047771632,
      "grad_norm": 2.0655901432037354,
      "learning_rate": 3.595225158704728e-05,
      "loss": 1.8392,
      "step": 60500
    },
    {
      "epoch": 0.8498307304364786,
      "grad_norm": 2.335900068283081,
      "learning_rate": 3.583615449272536e-05,
      "loss": 1.8205,
      "step": 61000
    },
    {
      "epoch": 0.8567965560957941,
      "grad_norm": 2.8406898975372314,
      "learning_rate": 3.572005739840344e-05,
      "loss": 1.8326,
      "step": 61500
    },
    {
      "epoch": 0.8637623817551094,
      "grad_norm": 2.251047372817993,
      "learning_rate": 3.560396030408151e-05,
      "loss": 1.8083,
      "step": 62000
    },
    {
      "epoch": 0.8707282074144248,
      "grad_norm": 2.539391279220581,
      "learning_rate": 3.5487863209759586e-05,
      "loss": 1.8157,
      "step": 62500
    },
    {
      "epoch": 0.8776940330737403,
      "grad_norm": 2.535517454147339,
      "learning_rate": 3.537176611543767e-05,
      "loss": 1.832,
      "step": 63000
    },
    {
      "epoch": 0.8846598587330556,
      "grad_norm": 2.1939809322357178,
      "learning_rate": 3.525566902111574e-05,
      "loss": 1.8351,
      "step": 63500
    },
    {
      "epoch": 0.891625684392371,
      "grad_norm": 2.8877975940704346,
      "learning_rate": 3.513957192679382e-05,
      "loss": 1.826,
      "step": 64000
    },
    {
      "epoch": 0.8985915100516865,
      "grad_norm": 3.487715482711792,
      "learning_rate": 3.502347483247189e-05,
      "loss": 1.8065,
      "step": 64500
    },
    {
      "epoch": 0.9055573357110018,
      "grad_norm": 2.618662118911743,
      "learning_rate": 3.490737773814997e-05,
      "loss": 1.8332,
      "step": 65000
    },
    {
      "epoch": 0.9125231613703172,
      "grad_norm": 3.937084197998047,
      "learning_rate": 3.479128064382805e-05,
      "loss": 1.8337,
      "step": 65500
    },
    {
      "epoch": 0.9194889870296327,
      "grad_norm": 3.074197769165039,
      "learning_rate": 3.467518354950612e-05,
      "loss": 1.8512,
      "step": 66000
    },
    {
      "epoch": 0.926454812688948,
      "grad_norm": 2.502962827682495,
      "learning_rate": 3.4559086455184204e-05,
      "loss": 1.8395,
      "step": 66500
    },
    {
      "epoch": 0.9334206383482634,
      "grad_norm": 2.3455824851989746,
      "learning_rate": 3.444298936086228e-05,
      "loss": 1.823,
      "step": 67000
    },
    {
      "epoch": 0.9403864640075789,
      "grad_norm": 2.203704357147217,
      "learning_rate": 3.432689226654035e-05,
      "loss": 1.8199,
      "step": 67500
    },
    {
      "epoch": 0.9473522896668942,
      "grad_norm": 1.8091837167739868,
      "learning_rate": 3.421079517221843e-05,
      "loss": 1.8259,
      "step": 68000
    },
    {
      "epoch": 0.9543181153262096,
      "grad_norm": 3.137481689453125,
      "learning_rate": 3.409469807789651e-05,
      "loss": 1.8318,
      "step": 68500
    },
    {
      "epoch": 0.961283940985525,
      "grad_norm": 2.5184240341186523,
      "learning_rate": 3.3978600983574584e-05,
      "loss": 1.8177,
      "step": 69000
    },
    {
      "epoch": 0.9682497666448404,
      "grad_norm": 2.9069859981536865,
      "learning_rate": 3.386250388925266e-05,
      "loss": 1.8246,
      "step": 69500
    },
    {
      "epoch": 0.9752155923041558,
      "grad_norm": 2.7795183658599854,
      "learning_rate": 3.374640679493074e-05,
      "loss": 1.8017,
      "step": 70000
    },
    {
      "epoch": 0.9821814179634712,
      "grad_norm": 2.4466419219970703,
      "learning_rate": 3.3630309700608815e-05,
      "loss": 1.8219,
      "step": 70500
    },
    {
      "epoch": 0.9891472436227866,
      "grad_norm": 2.2458741664886475,
      "learning_rate": 3.3514212606286896e-05,
      "loss": 1.8141,
      "step": 71000
    },
    {
      "epoch": 0.996113069282102,
      "grad_norm": 3.2624447345733643,
      "learning_rate": 3.3398115511964964e-05,
      "loss": 1.8209,
      "step": 71500
    },
    {
      "epoch": 1.0030788949414173,
      "grad_norm": 2.823734760284424,
      "learning_rate": 3.3282018417643046e-05,
      "loss": 1.8176,
      "step": 72000
    },
    {
      "epoch": 1.0100447206007328,
      "grad_norm": 2.21722412109375,
      "learning_rate": 3.316592132332112e-05,
      "loss": 1.8042,
      "step": 72500
    },
    {
      "epoch": 1.0170105462600483,
      "grad_norm": 2.502375364303589,
      "learning_rate": 3.30498242289992e-05,
      "loss": 1.7895,
      "step": 73000
    },
    {
      "epoch": 1.0239763719193635,
      "grad_norm": 2.975393056869507,
      "learning_rate": 3.293372713467727e-05,
      "loss": 1.7926,
      "step": 73500
    },
    {
      "epoch": 1.030942197578679,
      "grad_norm": 1.9806519746780396,
      "learning_rate": 3.281763004035535e-05,
      "loss": 1.8078,
      "step": 74000
    },
    {
      "epoch": 1.0379080232379945,
      "grad_norm": 3.024549961090088,
      "learning_rate": 3.270153294603343e-05,
      "loss": 1.7931,
      "step": 74500
    },
    {
      "epoch": 1.0448738488973097,
      "grad_norm": 2.0580055713653564,
      "learning_rate": 3.258543585171151e-05,
      "loss": 1.8005,
      "step": 75000
    },
    {
      "epoch": 1.0518396745566252,
      "grad_norm": 2.5066168308258057,
      "learning_rate": 3.246933875738958e-05,
      "loss": 1.799,
      "step": 75500
    },
    {
      "epoch": 1.0588055002159407,
      "grad_norm": 2.580336809158325,
      "learning_rate": 3.2353241663067656e-05,
      "loss": 1.7729,
      "step": 76000
    },
    {
      "epoch": 1.065771325875256,
      "grad_norm": 2.2235846519470215,
      "learning_rate": 3.223714456874574e-05,
      "loss": 1.8059,
      "step": 76500
    },
    {
      "epoch": 1.0727371515345714,
      "grad_norm": 2.348045587539673,
      "learning_rate": 3.212104747442381e-05,
      "loss": 1.7842,
      "step": 77000
    },
    {
      "epoch": 1.0797029771938869,
      "grad_norm": 2.713047504425049,
      "learning_rate": 3.200495038010189e-05,
      "loss": 1.816,
      "step": 77500
    },
    {
      "epoch": 1.086668802853202,
      "grad_norm": 1.9972302913665771,
      "learning_rate": 3.188885328577996e-05,
      "loss": 1.8054,
      "step": 78000
    },
    {
      "epoch": 1.0936346285125176,
      "grad_norm": 2.131059408187866,
      "learning_rate": 3.177275619145804e-05,
      "loss": 1.7938,
      "step": 78500
    },
    {
      "epoch": 1.100600454171833,
      "grad_norm": 2.161198139190674,
      "learning_rate": 3.165665909713612e-05,
      "loss": 1.7658,
      "step": 79000
    },
    {
      "epoch": 1.1075662798311483,
      "grad_norm": 2.677123785018921,
      "learning_rate": 3.154056200281419e-05,
      "loss": 1.8084,
      "step": 79500
    },
    {
      "epoch": 1.1145321054904638,
      "grad_norm": 2.2555668354034424,
      "learning_rate": 3.1424464908492274e-05,
      "loss": 1.824,
      "step": 80000
    },
    {
      "epoch": 1.1214979311497792,
      "grad_norm": 2.7274796962738037,
      "learning_rate": 3.130836781417035e-05,
      "loss": 1.7842,
      "step": 80500
    },
    {
      "epoch": 1.1284637568090945,
      "grad_norm": 2.1140997409820557,
      "learning_rate": 3.1192270719848423e-05,
      "loss": 1.8156,
      "step": 81000
    },
    {
      "epoch": 1.13542958246841,
      "grad_norm": 2.18180513381958,
      "learning_rate": 3.10761736255265e-05,
      "loss": 1.8157,
      "step": 81500
    },
    {
      "epoch": 1.1423954081277254,
      "grad_norm": 2.226754665374756,
      "learning_rate": 3.096007653120458e-05,
      "loss": 1.8049,
      "step": 82000
    },
    {
      "epoch": 1.1493612337870407,
      "grad_norm": 2.4654736518859863,
      "learning_rate": 3.0843979436882654e-05,
      "loss": 1.7819,
      "step": 82500
    },
    {
      "epoch": 1.1563270594463562,
      "grad_norm": 2.7540974617004395,
      "learning_rate": 3.072788234256073e-05,
      "loss": 1.7901,
      "step": 83000
    },
    {
      "epoch": 1.1632928851056716,
      "grad_norm": 2.5931143760681152,
      "learning_rate": 3.061178524823881e-05,
      "loss": 1.8003,
      "step": 83500
    },
    {
      "epoch": 1.1702587107649869,
      "grad_norm": 2.2241928577423096,
      "learning_rate": 3.0495688153916885e-05,
      "loss": 1.7923,
      "step": 84000
    },
    {
      "epoch": 1.1772245364243024,
      "grad_norm": 2.8928563594818115,
      "learning_rate": 3.0379591059594963e-05,
      "loss": 1.7958,
      "step": 84500
    },
    {
      "epoch": 1.1841903620836178,
      "grad_norm": 2.0658156871795654,
      "learning_rate": 3.0263493965273038e-05,
      "loss": 1.811,
      "step": 85000
    },
    {
      "epoch": 1.191156187742933,
      "grad_norm": 2.5697975158691406,
      "learning_rate": 3.0147396870951116e-05,
      "loss": 1.7898,
      "step": 85500
    },
    {
      "epoch": 1.1981220134022486,
      "grad_norm": 2.514228105545044,
      "learning_rate": 3.003129977662919e-05,
      "loss": 1.8255,
      "step": 86000
    },
    {
      "epoch": 1.205087839061564,
      "grad_norm": 2.320559501647949,
      "learning_rate": 2.991520268230727e-05,
      "loss": 1.7943,
      "step": 86500
    },
    {
      "epoch": 1.2120536647208793,
      "grad_norm": 2.6999759674072266,
      "learning_rate": 2.9799105587985343e-05,
      "loss": 1.7717,
      "step": 87000
    },
    {
      "epoch": 1.2190194903801947,
      "grad_norm": 2.599395513534546,
      "learning_rate": 2.968300849366342e-05,
      "loss": 1.8098,
      "step": 87500
    },
    {
      "epoch": 1.2259853160395102,
      "grad_norm": 2.287473201751709,
      "learning_rate": 2.9566911399341503e-05,
      "loss": 1.8187,
      "step": 88000
    },
    {
      "epoch": 1.2329511416988255,
      "grad_norm": 2.0074925422668457,
      "learning_rate": 2.9450814305019574e-05,
      "loss": 1.7776,
      "step": 88500
    },
    {
      "epoch": 1.239916967358141,
      "grad_norm": 2.0603387355804443,
      "learning_rate": 2.9334717210697655e-05,
      "loss": 1.8041,
      "step": 89000
    },
    {
      "epoch": 1.2468827930174564,
      "grad_norm": 2.055168390274048,
      "learning_rate": 2.9218620116375727e-05,
      "loss": 1.7874,
      "step": 89500
    },
    {
      "epoch": 1.2538486186767717,
      "grad_norm": 2.5835163593292236,
      "learning_rate": 2.9102523022053808e-05,
      "loss": 1.8118,
      "step": 90000
    },
    {
      "epoch": 1.2608144443360871,
      "grad_norm": 2.320622444152832,
      "learning_rate": 2.898642592773188e-05,
      "loss": 1.7735,
      "step": 90500
    },
    {
      "epoch": 1.2677802699954026,
      "grad_norm": 2.7421793937683105,
      "learning_rate": 2.887032883340996e-05,
      "loss": 1.8074,
      "step": 91000
    },
    {
      "epoch": 1.2747460956547179,
      "grad_norm": 2.4878551959991455,
      "learning_rate": 2.8754231739088032e-05,
      "loss": 1.7772,
      "step": 91500
    },
    {
      "epoch": 1.2817119213140333,
      "grad_norm": 1.7221839427947998,
      "learning_rate": 2.8638134644766114e-05,
      "loss": 1.7794,
      "step": 92000
    },
    {
      "epoch": 1.2886777469733488,
      "grad_norm": 2.2648704051971436,
      "learning_rate": 2.8522037550444185e-05,
      "loss": 1.7812,
      "step": 92500
    },
    {
      "epoch": 1.295643572632664,
      "grad_norm": 2.4838664531707764,
      "learning_rate": 2.8405940456122266e-05,
      "loss": 1.7992,
      "step": 93000
    },
    {
      "epoch": 1.3026093982919795,
      "grad_norm": 1.8448903560638428,
      "learning_rate": 2.8289843361800344e-05,
      "loss": 1.8209,
      "step": 93500
    },
    {
      "epoch": 1.309575223951295,
      "grad_norm": 2.562264919281006,
      "learning_rate": 2.817374626747842e-05,
      "loss": 1.7913,
      "step": 94000
    },
    {
      "epoch": 1.3165410496106102,
      "grad_norm": 2.56683611869812,
      "learning_rate": 2.8057649173156497e-05,
      "loss": 1.7588,
      "step": 94500
    },
    {
      "epoch": 1.3235068752699257,
      "grad_norm": 2.7661664485931396,
      "learning_rate": 2.794155207883457e-05,
      "loss": 1.8096,
      "step": 95000
    },
    {
      "epoch": 1.3304727009292412,
      "grad_norm": 2.556074857711792,
      "learning_rate": 2.782545498451265e-05,
      "loss": 1.7846,
      "step": 95500
    },
    {
      "epoch": 1.3374385265885564,
      "grad_norm": 2.0837624073028564,
      "learning_rate": 2.7709357890190724e-05,
      "loss": 1.793,
      "step": 96000
    },
    {
      "epoch": 1.344404352247872,
      "grad_norm": 3.0657904148101807,
      "learning_rate": 2.7593260795868802e-05,
      "loss": 1.7705,
      "step": 96500
    },
    {
      "epoch": 1.3513701779071874,
      "grad_norm": 2.1145811080932617,
      "learning_rate": 2.7477163701546877e-05,
      "loss": 1.8045,
      "step": 97000
    },
    {
      "epoch": 1.3583360035665026,
      "grad_norm": 2.3188581466674805,
      "learning_rate": 2.7361066607224955e-05,
      "loss": 1.8031,
      "step": 97500
    },
    {
      "epoch": 1.365301829225818,
      "grad_norm": 2.250185012817383,
      "learning_rate": 2.7244969512903033e-05,
      "loss": 1.7963,
      "step": 98000
    },
    {
      "epoch": 1.3722676548851336,
      "grad_norm": 5.883315563201904,
      "learning_rate": 2.7128872418581108e-05,
      "loss": 1.7912,
      "step": 98500
    },
    {
      "epoch": 1.3792334805444488,
      "grad_norm": 1.9139126539230347,
      "learning_rate": 2.7012775324259186e-05,
      "loss": 1.7754,
      "step": 99000
    },
    {
      "epoch": 1.3861993062037643,
      "grad_norm": 2.3499364852905273,
      "learning_rate": 2.689667822993726e-05,
      "loss": 1.7883,
      "step": 99500
    },
    {
      "epoch": 1.3931651318630798,
      "grad_norm": 2.7324042320251465,
      "learning_rate": 2.678058113561534e-05,
      "loss": 1.817,
      "step": 100000
    },
    {
      "epoch": 1.400130957522395,
      "grad_norm": 2.347512722015381,
      "learning_rate": 2.6664484041293413e-05,
      "loss": 1.8084,
      "step": 100500
    },
    {
      "epoch": 1.4070967831817105,
      "grad_norm": 2.0082993507385254,
      "learning_rate": 2.654838694697149e-05,
      "loss": 1.8022,
      "step": 101000
    },
    {
      "epoch": 1.414062608841026,
      "grad_norm": 2.8789451122283936,
      "learning_rate": 2.6432289852649566e-05,
      "loss": 1.8034,
      "step": 101500
    },
    {
      "epoch": 1.4210284345003412,
      "grad_norm": 2.5812363624572754,
      "learning_rate": 2.6316192758327644e-05,
      "loss": 1.803,
      "step": 102000
    },
    {
      "epoch": 1.4279942601596567,
      "grad_norm": 2.2051737308502197,
      "learning_rate": 2.6200095664005726e-05,
      "loss": 1.7995,
      "step": 102500
    },
    {
      "epoch": 1.4349600858189722,
      "grad_norm": 2.773801326751709,
      "learning_rate": 2.6083998569683797e-05,
      "loss": 1.8354,
      "step": 103000
    },
    {
      "epoch": 1.4419259114782874,
      "grad_norm": 2.3009896278381348,
      "learning_rate": 2.5967901475361878e-05,
      "loss": 1.7918,
      "step": 103500
    },
    {
      "epoch": 1.4488917371376029,
      "grad_norm": 2.4160633087158203,
      "learning_rate": 2.5851804381039953e-05,
      "loss": 1.8052,
      "step": 104000
    },
    {
      "epoch": 1.4558575627969184,
      "grad_norm": 2.925910234451294,
      "learning_rate": 2.573570728671803e-05,
      "loss": 1.7984,
      "step": 104500
    },
    {
      "epoch": 1.4628233884562336,
      "grad_norm": 2.212982654571533,
      "learning_rate": 2.5619610192396106e-05,
      "loss": 1.7902,
      "step": 105000
    },
    {
      "epoch": 1.469789214115549,
      "grad_norm": 2.1370463371276855,
      "learning_rate": 2.5503513098074184e-05,
      "loss": 1.7812,
      "step": 105500
    },
    {
      "epoch": 1.4767550397748646,
      "grad_norm": 2.6459648609161377,
      "learning_rate": 2.538741600375226e-05,
      "loss": 1.794,
      "step": 106000
    },
    {
      "epoch": 1.4837208654341798,
      "grad_norm": 2.117462158203125,
      "learning_rate": 2.5271318909430336e-05,
      "loss": 1.7945,
      "step": 106500
    },
    {
      "epoch": 1.4906866910934953,
      "grad_norm": 2.504960060119629,
      "learning_rate": 2.5155221815108414e-05,
      "loss": 1.7707,
      "step": 107000
    },
    {
      "epoch": 1.4976525167528107,
      "grad_norm": 2.5757598876953125,
      "learning_rate": 2.503912472078649e-05,
      "loss": 1.7879,
      "step": 107500
    },
    {
      "epoch": 1.504618342412126,
      "grad_norm": 2.738373041152954,
      "learning_rate": 2.4923027626464564e-05,
      "loss": 1.7996,
      "step": 108000
    },
    {
      "epoch": 1.5115841680714415,
      "grad_norm": 1.8770989179611206,
      "learning_rate": 2.4806930532142642e-05,
      "loss": 1.7951,
      "step": 108500
    },
    {
      "epoch": 1.518549993730757,
      "grad_norm": 2.6828932762145996,
      "learning_rate": 2.469083343782072e-05,
      "loss": 1.784,
      "step": 109000
    },
    {
      "epoch": 1.5255158193900722,
      "grad_norm": 2.6234092712402344,
      "learning_rate": 2.4574736343498798e-05,
      "loss": 1.7877,
      "step": 109500
    },
    {
      "epoch": 1.5324816450493877,
      "grad_norm": 2.8595707416534424,
      "learning_rate": 2.4458639249176873e-05,
      "loss": 1.804,
      "step": 110000
    },
    {
      "epoch": 1.5394474707087031,
      "grad_norm": 2.548415184020996,
      "learning_rate": 2.434254215485495e-05,
      "loss": 1.7935,
      "step": 110500
    },
    {
      "epoch": 1.5464132963680184,
      "grad_norm": 2.706181526184082,
      "learning_rate": 2.4226445060533025e-05,
      "loss": 1.78,
      "step": 111000
    },
    {
      "epoch": 1.5533791220273339,
      "grad_norm": 2.350249767303467,
      "learning_rate": 2.4110347966211103e-05,
      "loss": 1.7955,
      "step": 111500
    },
    {
      "epoch": 1.5603449476866493,
      "grad_norm": 2.8380746841430664,
      "learning_rate": 2.3994250871889178e-05,
      "loss": 1.8007,
      "step": 112000
    },
    {
      "epoch": 1.5673107733459646,
      "grad_norm": 2.8403515815734863,
      "learning_rate": 2.3878153777567256e-05,
      "loss": 1.8078,
      "step": 112500
    },
    {
      "epoch": 1.57427659900528,
      "grad_norm": 2.2605979442596436,
      "learning_rate": 2.376205668324533e-05,
      "loss": 1.7803,
      "step": 113000
    },
    {
      "epoch": 1.5812424246645955,
      "grad_norm": 2.617479085922241,
      "learning_rate": 2.3645959588923412e-05,
      "loss": 1.7824,
      "step": 113500
    },
    {
      "epoch": 1.5882082503239108,
      "grad_norm": 2.201807975769043,
      "learning_rate": 2.3529862494601487e-05,
      "loss": 1.7927,
      "step": 114000
    },
    {
      "epoch": 1.5951740759832262,
      "grad_norm": 2.542644500732422,
      "learning_rate": 2.3413765400279565e-05,
      "loss": 1.8054,
      "step": 114500
    },
    {
      "epoch": 1.6021399016425417,
      "grad_norm": 1.7047696113586426,
      "learning_rate": 2.329766830595764e-05,
      "loss": 1.8267,
      "step": 115000
    },
    {
      "epoch": 1.609105727301857,
      "grad_norm": 1.9278171062469482,
      "learning_rate": 2.3181571211635718e-05,
      "loss": 1.8091,
      "step": 115500
    },
    {
      "epoch": 1.6160715529611724,
      "grad_norm": 1.9207502603530884,
      "learning_rate": 2.3065474117313792e-05,
      "loss": 1.7889,
      "step": 116000
    },
    {
      "epoch": 1.623037378620488,
      "grad_norm": 2.7679696083068848,
      "learning_rate": 2.294937702299187e-05,
      "loss": 1.7982,
      "step": 116500
    },
    {
      "epoch": 1.6300032042798032,
      "grad_norm": 3.063542366027832,
      "learning_rate": 2.2833279928669945e-05,
      "loss": 1.7765,
      "step": 117000
    },
    {
      "epoch": 1.6369690299391186,
      "grad_norm": 2.278334856033325,
      "learning_rate": 2.2717182834348023e-05,
      "loss": 1.7807,
      "step": 117500
    },
    {
      "epoch": 1.643934855598434,
      "grad_norm": 2.50698184967041,
      "learning_rate": 2.2601085740026098e-05,
      "loss": 1.7864,
      "step": 118000
    },
    {
      "epoch": 1.6509006812577494,
      "grad_norm": 2.4421353340148926,
      "learning_rate": 2.2484988645704176e-05,
      "loss": 1.7818,
      "step": 118500
    },
    {
      "epoch": 1.6578665069170648,
      "grad_norm": 1.9500129222869873,
      "learning_rate": 2.2368891551382254e-05,
      "loss": 1.7756,
      "step": 119000
    },
    {
      "epoch": 1.6648323325763803,
      "grad_norm": 2.6357312202453613,
      "learning_rate": 2.2252794457060332e-05,
      "loss": 1.8095,
      "step": 119500
    },
    {
      "epoch": 1.6717981582356956,
      "grad_norm": 2.3673174381256104,
      "learning_rate": 2.2136697362738407e-05,
      "loss": 1.7762,
      "step": 120000
    },
    {
      "epoch": 1.678763983895011,
      "grad_norm": 1.8235833644866943,
      "learning_rate": 2.2020600268416485e-05,
      "loss": 1.7941,
      "step": 120500
    },
    {
      "epoch": 1.6857298095543265,
      "grad_norm": 2.1190755367279053,
      "learning_rate": 2.190450317409456e-05,
      "loss": 1.793,
      "step": 121000
    },
    {
      "epoch": 1.6926956352136417,
      "grad_norm": 2.0466208457946777,
      "learning_rate": 2.1788406079772637e-05,
      "loss": 1.7881,
      "step": 121500
    },
    {
      "epoch": 1.6996614608729572,
      "grad_norm": 2.7461092472076416,
      "learning_rate": 2.1672308985450712e-05,
      "loss": 1.7893,
      "step": 122000
    },
    {
      "epoch": 1.7066272865322727,
      "grad_norm": 1.9035996198654175,
      "learning_rate": 2.155621189112879e-05,
      "loss": 1.7949,
      "step": 122500
    },
    {
      "epoch": 1.713593112191588,
      "grad_norm": 2.7511632442474365,
      "learning_rate": 2.1440114796806868e-05,
      "loss": 1.7695,
      "step": 123000
    },
    {
      "epoch": 1.7205589378509034,
      "grad_norm": 2.399855613708496,
      "learning_rate": 2.1324017702484943e-05,
      "loss": 1.78,
      "step": 123500
    },
    {
      "epoch": 1.7275247635102189,
      "grad_norm": 2.634815216064453,
      "learning_rate": 2.120792060816302e-05,
      "loss": 1.7841,
      "step": 124000
    },
    {
      "epoch": 1.7344905891695341,
      "grad_norm": 2.3935928344726562,
      "learning_rate": 2.1091823513841096e-05,
      "loss": 1.8267,
      "step": 124500
    },
    {
      "epoch": 1.7414564148288496,
      "grad_norm": 2.5382657051086426,
      "learning_rate": 2.0975726419519174e-05,
      "loss": 1.7902,
      "step": 125000
    },
    {
      "epoch": 1.748422240488165,
      "grad_norm": 2.3007774353027344,
      "learning_rate": 2.0859629325197248e-05,
      "loss": 1.7739,
      "step": 125500
    },
    {
      "epoch": 1.7553880661474803,
      "grad_norm": 2.3945631980895996,
      "learning_rate": 2.0743532230875326e-05,
      "loss": 1.799,
      "step": 126000
    },
    {
      "epoch": 1.762353891806796,
      "grad_norm": 2.8447561264038086,
      "learning_rate": 2.06274351365534e-05,
      "loss": 1.7922,
      "step": 126500
    },
    {
      "epoch": 1.7693197174661113,
      "grad_norm": 2.309450626373291,
      "learning_rate": 2.051133804223148e-05,
      "loss": 1.8058,
      "step": 127000
    },
    {
      "epoch": 1.7762855431254265,
      "grad_norm": 2.272334575653076,
      "learning_rate": 2.0395240947909557e-05,
      "loss": 1.7958,
      "step": 127500
    },
    {
      "epoch": 1.7832513687847422,
      "grad_norm": 1.811288595199585,
      "learning_rate": 2.0279143853587635e-05,
      "loss": 1.828,
      "step": 128000
    },
    {
      "epoch": 1.7902171944440575,
      "grad_norm": 2.157273292541504,
      "learning_rate": 2.016304675926571e-05,
      "loss": 1.79,
      "step": 128500
    },
    {
      "epoch": 1.7971830201033727,
      "grad_norm": 2.2948687076568604,
      "learning_rate": 2.0046949664943788e-05,
      "loss": 1.807,
      "step": 129000
    },
    {
      "epoch": 1.8041488457626884,
      "grad_norm": 2.4647552967071533,
      "learning_rate": 1.9930852570621863e-05,
      "loss": 1.7766,
      "step": 129500
    },
    {
      "epoch": 1.8111146714220037,
      "grad_norm": 2.7788565158843994,
      "learning_rate": 1.981475547629994e-05,
      "loss": 1.7839,
      "step": 130000
    },
    {
      "epoch": 1.818080497081319,
      "grad_norm": 2.4632039070129395,
      "learning_rate": 1.9698658381978015e-05,
      "loss": 1.8144,
      "step": 130500
    },
    {
      "epoch": 1.8250463227406346,
      "grad_norm": 1.799288034439087,
      "learning_rate": 1.9582561287656093e-05,
      "loss": 1.7929,
      "step": 131000
    },
    {
      "epoch": 1.8320121483999499,
      "grad_norm": 2.348379373550415,
      "learning_rate": 1.9466464193334168e-05,
      "loss": 1.7907,
      "step": 131500
    },
    {
      "epoch": 1.838977974059265,
      "grad_norm": 2.087851047515869,
      "learning_rate": 1.935036709901225e-05,
      "loss": 1.7856,
      "step": 132000
    },
    {
      "epoch": 1.8459437997185808,
      "grad_norm": 2.3005926609039307,
      "learning_rate": 1.9234270004690324e-05,
      "loss": 1.7817,
      "step": 132500
    },
    {
      "epoch": 1.852909625377896,
      "grad_norm": 3.215254783630371,
      "learning_rate": 1.9118172910368402e-05,
      "loss": 1.8004,
      "step": 133000
    },
    {
      "epoch": 1.8598754510372113,
      "grad_norm": 2.6516568660736084,
      "learning_rate": 1.9002075816046477e-05,
      "loss": 1.8099,
      "step": 133500
    },
    {
      "epoch": 1.866841276696527,
      "grad_norm": 2.640639305114746,
      "learning_rate": 1.8885978721724555e-05,
      "loss": 1.7846,
      "step": 134000
    },
    {
      "epoch": 1.8738071023558422,
      "grad_norm": 2.315358877182007,
      "learning_rate": 1.876988162740263e-05,
      "loss": 1.7835,
      "step": 134500
    },
    {
      "epoch": 1.8807729280151575,
      "grad_norm": 1.8584767580032349,
      "learning_rate": 1.8653784533080708e-05,
      "loss": 1.7664,
      "step": 135000
    },
    {
      "epoch": 1.8877387536744732,
      "grad_norm": 3.296114206314087,
      "learning_rate": 1.8537687438758782e-05,
      "loss": 1.7833,
      "step": 135500
    },
    {
      "epoch": 1.8947045793337884,
      "grad_norm": 2.1423351764678955,
      "learning_rate": 1.842159034443686e-05,
      "loss": 1.8111,
      "step": 136000
    },
    {
      "epoch": 1.9016704049931037,
      "grad_norm": 2.367438316345215,
      "learning_rate": 1.8305493250114935e-05,
      "loss": 1.8095,
      "step": 136500
    },
    {
      "epoch": 1.9086362306524194,
      "grad_norm": 3.009122610092163,
      "learning_rate": 1.8189396155793016e-05,
      "loss": 1.7651,
      "step": 137000
    },
    {
      "epoch": 1.9156020563117346,
      "grad_norm": 2.2785120010375977,
      "learning_rate": 1.807329906147109e-05,
      "loss": 1.7754,
      "step": 137500
    },
    {
      "epoch": 1.9225678819710499,
      "grad_norm": 2.341623544692993,
      "learning_rate": 1.795720196714917e-05,
      "loss": 1.8049,
      "step": 138000
    },
    {
      "epoch": 1.9295337076303656,
      "grad_norm": 2.859114408493042,
      "learning_rate": 1.7841104872827244e-05,
      "loss": 1.7812,
      "step": 138500
    },
    {
      "epoch": 1.9364995332896808,
      "grad_norm": 2.100924015045166,
      "learning_rate": 1.7725007778505322e-05,
      "loss": 1.7875,
      "step": 139000
    },
    {
      "epoch": 1.943465358948996,
      "grad_norm": 3.360846757888794,
      "learning_rate": 1.7608910684183396e-05,
      "loss": 1.8099,
      "step": 139500
    },
    {
      "epoch": 1.9504311846083118,
      "grad_norm": 2.3364663124084473,
      "learning_rate": 1.7492813589861475e-05,
      "loss": 1.7936,
      "step": 140000
    },
    {
      "epoch": 1.957397010267627,
      "grad_norm": 2.3441708087921143,
      "learning_rate": 1.737671649553955e-05,
      "loss": 1.7842,
      "step": 140500
    },
    {
      "epoch": 1.9643628359269423,
      "grad_norm": 3.181539535522461,
      "learning_rate": 1.7260619401217627e-05,
      "loss": 1.7834,
      "step": 141000
    },
    {
      "epoch": 1.971328661586258,
      "grad_norm": 2.0510308742523193,
      "learning_rate": 1.7144522306895705e-05,
      "loss": 1.8078,
      "step": 141500
    },
    {
      "epoch": 1.9782944872455732,
      "grad_norm": 2.195309638977051,
      "learning_rate": 1.702842521257378e-05,
      "loss": 1.7952,
      "step": 142000
    },
    {
      "epoch": 1.9852603129048885,
      "grad_norm": 2.048460006713867,
      "learning_rate": 1.6912328118251858e-05,
      "loss": 1.769,
      "step": 142500
    },
    {
      "epoch": 1.9922261385642042,
      "grad_norm": 2.2277722358703613,
      "learning_rate": 1.6796231023929933e-05,
      "loss": 1.8041,
      "step": 143000
    },
    {
      "epoch": 1.9991919642235194,
      "grad_norm": 2.1303534507751465,
      "learning_rate": 1.668013392960801e-05,
      "loss": 1.805,
      "step": 143500
    },
    {
      "epoch": 2.0061577898828347,
      "grad_norm": 2.7264699935913086,
      "learning_rate": 1.6564036835286085e-05,
      "loss": 1.8004,
      "step": 144000
    },
    {
      "epoch": 2.0131236155421504,
      "grad_norm": 1.8580347299575806,
      "learning_rate": 1.6447939740964163e-05,
      "loss": 1.7404,
      "step": 144500
    },
    {
      "epoch": 2.0200894412014656,
      "grad_norm": 2.159956216812134,
      "learning_rate": 1.6331842646642238e-05,
      "loss": 1.7671,
      "step": 145000
    },
    {
      "epoch": 2.027055266860781,
      "grad_norm": 2.3722689151763916,
      "learning_rate": 1.6215745552320316e-05,
      "loss": 1.7598,
      "step": 145500
    },
    {
      "epoch": 2.0340210925200966,
      "grad_norm": 2.8019232749938965,
      "learning_rate": 1.6099648457998394e-05,
      "loss": 1.78,
      "step": 146000
    },
    {
      "epoch": 2.040986918179412,
      "grad_norm": 2.4757485389709473,
      "learning_rate": 1.5983551363676472e-05,
      "loss": 1.7827,
      "step": 146500
    },
    {
      "epoch": 2.047952743838727,
      "grad_norm": 1.8825523853302002,
      "learning_rate": 1.5867454269354547e-05,
      "loss": 1.7792,
      "step": 147000
    },
    {
      "epoch": 2.0549185694980427,
      "grad_norm": 2.2722554206848145,
      "learning_rate": 1.5751357175032625e-05,
      "loss": 1.7867,
      "step": 147500
    },
    {
      "epoch": 2.061884395157358,
      "grad_norm": 2.078244209289551,
      "learning_rate": 1.56352600807107e-05,
      "loss": 1.7812,
      "step": 148000
    },
    {
      "epoch": 2.0688502208166732,
      "grad_norm": 2.4361000061035156,
      "learning_rate": 1.5519162986388778e-05,
      "loss": 1.7524,
      "step": 148500
    },
    {
      "epoch": 2.075816046475989,
      "grad_norm": 2.385258436203003,
      "learning_rate": 1.5403065892066852e-05,
      "loss": 1.7705,
      "step": 149000
    },
    {
      "epoch": 2.082781872135304,
      "grad_norm": 2.102510690689087,
      "learning_rate": 1.528696879774493e-05,
      "loss": 1.7869,
      "step": 149500
    },
    {
      "epoch": 2.0897476977946194,
      "grad_norm": 2.186758041381836,
      "learning_rate": 1.5170871703423007e-05,
      "loss": 1.7604,
      "step": 150000
    },
    {
      "epoch": 2.096713523453935,
      "grad_norm": 2.4460811614990234,
      "learning_rate": 1.5054774609101085e-05,
      "loss": 1.7714,
      "step": 150500
    },
    {
      "epoch": 2.1036793491132504,
      "grad_norm": 2.7168900966644287,
      "learning_rate": 1.4938677514779161e-05,
      "loss": 1.7661,
      "step": 151000
    },
    {
      "epoch": 2.1106451747725656,
      "grad_norm": 3.0791850090026855,
      "learning_rate": 1.4822580420457238e-05,
      "loss": 1.7617,
      "step": 151500
    },
    {
      "epoch": 2.1176110004318813,
      "grad_norm": 3.055875301361084,
      "learning_rate": 1.4706483326135314e-05,
      "loss": 1.7734,
      "step": 152000
    },
    {
      "epoch": 2.1245768260911966,
      "grad_norm": 2.9692306518554688,
      "learning_rate": 1.459038623181339e-05,
      "loss": 1.7711,
      "step": 152500
    },
    {
      "epoch": 2.131542651750512,
      "grad_norm": 2.408766984939575,
      "learning_rate": 1.4474289137491467e-05,
      "loss": 1.8104,
      "step": 153000
    },
    {
      "epoch": 2.1385084774098275,
      "grad_norm": 3.5362493991851807,
      "learning_rate": 1.4358192043169543e-05,
      "loss": 1.7777,
      "step": 153500
    },
    {
      "epoch": 2.1454743030691428,
      "grad_norm": 2.0941126346588135,
      "learning_rate": 1.424209494884762e-05,
      "loss": 1.7853,
      "step": 154000
    },
    {
      "epoch": 2.152440128728458,
      "grad_norm": 1.951426386833191,
      "learning_rate": 1.4125997854525696e-05,
      "loss": 1.7562,
      "step": 154500
    },
    {
      "epoch": 2.1594059543877737,
      "grad_norm": 2.4422152042388916,
      "learning_rate": 1.4009900760203774e-05,
      "loss": 1.7814,
      "step": 155000
    },
    {
      "epoch": 2.166371780047089,
      "grad_norm": 2.6165547370910645,
      "learning_rate": 1.3893803665881852e-05,
      "loss": 1.7908,
      "step": 155500
    },
    {
      "epoch": 2.173337605706404,
      "grad_norm": 2.1683154106140137,
      "learning_rate": 1.3777706571559928e-05,
      "loss": 1.7804,
      "step": 156000
    },
    {
      "epoch": 2.18030343136572,
      "grad_norm": 2.957003116607666,
      "learning_rate": 1.3661609477238005e-05,
      "loss": 1.7711,
      "step": 156500
    },
    {
      "epoch": 2.187269257025035,
      "grad_norm": 2.016043186187744,
      "learning_rate": 1.3545512382916081e-05,
      "loss": 1.7668,
      "step": 157000
    },
    {
      "epoch": 2.1942350826843504,
      "grad_norm": 1.7968798875808716,
      "learning_rate": 1.3429415288594157e-05,
      "loss": 1.7745,
      "step": 157500
    },
    {
      "epoch": 2.201200908343666,
      "grad_norm": 3.06266450881958,
      "learning_rate": 1.3313318194272234e-05,
      "loss": 1.7837,
      "step": 158000
    },
    {
      "epoch": 2.2081667340029814,
      "grad_norm": 2.3120017051696777,
      "learning_rate": 1.319722109995031e-05,
      "loss": 1.7837,
      "step": 158500
    },
    {
      "epoch": 2.2151325596622966,
      "grad_norm": 2.2560901641845703,
      "learning_rate": 1.3081124005628386e-05,
      "loss": 1.7712,
      "step": 159000
    },
    {
      "epoch": 2.2220983853216123,
      "grad_norm": 2.0822031497955322,
      "learning_rate": 1.2965026911306463e-05,
      "loss": 1.7912,
      "step": 159500
    },
    {
      "epoch": 2.2290642109809276,
      "grad_norm": 2.1278085708618164,
      "learning_rate": 1.2848929816984542e-05,
      "loss": 1.7685,
      "step": 160000
    },
    {
      "epoch": 2.236030036640243,
      "grad_norm": 2.6186113357543945,
      "learning_rate": 1.2732832722662619e-05,
      "loss": 1.768,
      "step": 160500
    },
    {
      "epoch": 2.2429958622995585,
      "grad_norm": 2.5398612022399902,
      "learning_rate": 1.2616735628340695e-05,
      "loss": 1.7646,
      "step": 161000
    },
    {
      "epoch": 2.2499616879588737,
      "grad_norm": 2.8711392879486084,
      "learning_rate": 1.2500638534018772e-05,
      "loss": 1.767,
      "step": 161500
    },
    {
      "epoch": 2.256927513618189,
      "grad_norm": 2.9360668659210205,
      "learning_rate": 1.2384541439696848e-05,
      "loss": 1.7745,
      "step": 162000
    },
    {
      "epoch": 2.2638933392775047,
      "grad_norm": 2.1323444843292236,
      "learning_rate": 1.2268444345374924e-05,
      "loss": 1.7565,
      "step": 162500
    },
    {
      "epoch": 2.27085916493682,
      "grad_norm": 2.1959497928619385,
      "learning_rate": 1.2152347251053e-05,
      "loss": 1.7918,
      "step": 163000
    },
    {
      "epoch": 2.277824990596135,
      "grad_norm": 2.872119665145874,
      "learning_rate": 1.2036250156731079e-05,
      "loss": 1.7485,
      "step": 163500
    },
    {
      "epoch": 2.284790816255451,
      "grad_norm": 13.167923927307129,
      "learning_rate": 1.1920153062409155e-05,
      "loss": 1.7571,
      "step": 164000
    },
    {
      "epoch": 2.291756641914766,
      "grad_norm": 2.650646924972534,
      "learning_rate": 1.1804055968087231e-05,
      "loss": 1.7908,
      "step": 164500
    },
    {
      "epoch": 2.2987224675740814,
      "grad_norm": 2.9731333255767822,
      "learning_rate": 1.1687958873765308e-05,
      "loss": 1.7874,
      "step": 165000
    },
    {
      "epoch": 2.305688293233397,
      "grad_norm": 2.954777717590332,
      "learning_rate": 1.1571861779443384e-05,
      "loss": 1.7811,
      "step": 165500
    },
    {
      "epoch": 2.3126541188927123,
      "grad_norm": 2.2229397296905518,
      "learning_rate": 1.1455764685121462e-05,
      "loss": 1.7919,
      "step": 166000
    },
    {
      "epoch": 2.3196199445520276,
      "grad_norm": 1.8139567375183105,
      "learning_rate": 1.1339667590799539e-05,
      "loss": 1.7496,
      "step": 166500
    },
    {
      "epoch": 2.3265857702113433,
      "grad_norm": 2.9600841999053955,
      "learning_rate": 1.1223570496477615e-05,
      "loss": 1.7703,
      "step": 167000
    },
    {
      "epoch": 2.3335515958706585,
      "grad_norm": 2.3570051193237305,
      "learning_rate": 1.1107473402155691e-05,
      "loss": 1.7666,
      "step": 167500
    },
    {
      "epoch": 2.3405174215299738,
      "grad_norm": 2.224013090133667,
      "learning_rate": 1.099137630783377e-05,
      "loss": 1.7673,
      "step": 168000
    },
    {
      "epoch": 2.3474832471892895,
      "grad_norm": 2.8468456268310547,
      "learning_rate": 1.0875279213511846e-05,
      "loss": 1.7813,
      "step": 168500
    },
    {
      "epoch": 2.3544490728486047,
      "grad_norm": 2.634150743484497,
      "learning_rate": 1.0759182119189922e-05,
      "loss": 1.747,
      "step": 169000
    },
    {
      "epoch": 2.36141489850792,
      "grad_norm": 2.3928868770599365,
      "learning_rate": 1.0643085024867998e-05,
      "loss": 1.7694,
      "step": 169500
    },
    {
      "epoch": 2.3683807241672357,
      "grad_norm": 2.0316550731658936,
      "learning_rate": 1.0526987930546075e-05,
      "loss": 1.7729,
      "step": 170000
    },
    {
      "epoch": 2.375346549826551,
      "grad_norm": 2.8822085857391357,
      "learning_rate": 1.0410890836224153e-05,
      "loss": 1.7434,
      "step": 170500
    },
    {
      "epoch": 2.382312375485866,
      "grad_norm": 2.33028244972229,
      "learning_rate": 1.029479374190223e-05,
      "loss": 1.7833,
      "step": 171000
    },
    {
      "epoch": 2.389278201145182,
      "grad_norm": 2.313825845718384,
      "learning_rate": 1.0178696647580306e-05,
      "loss": 1.7887,
      "step": 171500
    },
    {
      "epoch": 2.396244026804497,
      "grad_norm": 2.850764513015747,
      "learning_rate": 1.0062599553258382e-05,
      "loss": 1.7557,
      "step": 172000
    },
    {
      "epoch": 2.4032098524638124,
      "grad_norm": 2.0915393829345703,
      "learning_rate": 9.946502458936458e-06,
      "loss": 1.7701,
      "step": 172500
    },
    {
      "epoch": 2.410175678123128,
      "grad_norm": 2.590662956237793,
      "learning_rate": 9.830405364614535e-06,
      "loss": 1.7782,
      "step": 173000
    },
    {
      "epoch": 2.4171415037824433,
      "grad_norm": 2.131538152694702,
      "learning_rate": 9.714308270292611e-06,
      "loss": 1.771,
      "step": 173500
    },
    {
      "epoch": 2.4241073294417586,
      "grad_norm": 3.607041120529175,
      "learning_rate": 9.598211175970687e-06,
      "loss": 1.7482,
      "step": 174000
    },
    {
      "epoch": 2.4310731551010742,
      "grad_norm": 2.2766165733337402,
      "learning_rate": 9.482114081648764e-06,
      "loss": 1.7482,
      "step": 174500
    },
    {
      "epoch": 2.4380389807603895,
      "grad_norm": 3.1811325550079346,
      "learning_rate": 9.366016987326842e-06,
      "loss": 1.7873,
      "step": 175000
    },
    {
      "epoch": 2.4450048064197047,
      "grad_norm": 2.062478542327881,
      "learning_rate": 9.249919893004918e-06,
      "loss": 1.7756,
      "step": 175500
    },
    {
      "epoch": 2.4519706320790204,
      "grad_norm": 2.2475478649139404,
      "learning_rate": 9.133822798682994e-06,
      "loss": 1.7856,
      "step": 176000
    },
    {
      "epoch": 2.4589364577383357,
      "grad_norm": 2.1175222396850586,
      "learning_rate": 9.01772570436107e-06,
      "loss": 1.766,
      "step": 176500
    },
    {
      "epoch": 2.465902283397651,
      "grad_norm": 2.2508065700531006,
      "learning_rate": 8.901628610039147e-06,
      "loss": 1.7526,
      "step": 177000
    },
    {
      "epoch": 2.4728681090569666,
      "grad_norm": 3.388170003890991,
      "learning_rate": 8.785531515717225e-06,
      "loss": 1.7741,
      "step": 177500
    },
    {
      "epoch": 2.479833934716282,
      "grad_norm": 2.58388614654541,
      "learning_rate": 8.669434421395302e-06,
      "loss": 1.7492,
      "step": 178000
    },
    {
      "epoch": 2.486799760375597,
      "grad_norm": 1.6890056133270264,
      "learning_rate": 8.553337327073378e-06,
      "loss": 1.7572,
      "step": 178500
    },
    {
      "epoch": 2.493765586034913,
      "grad_norm": 2.3476877212524414,
      "learning_rate": 8.437240232751454e-06,
      "loss": 1.7694,
      "step": 179000
    },
    {
      "epoch": 2.500731411694228,
      "grad_norm": 2.3599612712860107,
      "learning_rate": 8.321143138429532e-06,
      "loss": 1.7574,
      "step": 179500
    },
    {
      "epoch": 2.5076972373535433,
      "grad_norm": 2.3605785369873047,
      "learning_rate": 8.205046044107609e-06,
      "loss": 1.7708,
      "step": 180000
    },
    {
      "epoch": 2.514663063012859,
      "grad_norm": 2.747734785079956,
      "learning_rate": 8.088948949785685e-06,
      "loss": 1.7908,
      "step": 180500
    },
    {
      "epoch": 2.5216288886721743,
      "grad_norm": 2.6813743114471436,
      "learning_rate": 7.972851855463761e-06,
      "loss": 1.734,
      "step": 181000
    },
    {
      "epoch": 2.52859471433149,
      "grad_norm": 1.9836755990982056,
      "learning_rate": 7.856754761141838e-06,
      "loss": 1.7597,
      "step": 181500
    },
    {
      "epoch": 2.535560539990805,
      "grad_norm": 3.0161242485046387,
      "learning_rate": 7.740657666819916e-06,
      "loss": 1.756,
      "step": 182000
    },
    {
      "epoch": 2.5425263656501205,
      "grad_norm": 2.1362907886505127,
      "learning_rate": 7.624560572497992e-06,
      "loss": 1.7656,
      "step": 182500
    },
    {
      "epoch": 2.5494921913094357,
      "grad_norm": 2.5035154819488525,
      "learning_rate": 7.5084634781760686e-06,
      "loss": 1.7501,
      "step": 183000
    },
    {
      "epoch": 2.5564580169687514,
      "grad_norm": 1.9174439907073975,
      "learning_rate": 7.392366383854145e-06,
      "loss": 1.7689,
      "step": 183500
    },
    {
      "epoch": 2.5634238426280667,
      "grad_norm": 2.7090957164764404,
      "learning_rate": 7.276269289532221e-06,
      "loss": 1.7772,
      "step": 184000
    },
    {
      "epoch": 2.5703896682873824,
      "grad_norm": 2.32120680809021,
      "learning_rate": 7.160172195210299e-06,
      "loss": 1.778,
      "step": 184500
    },
    {
      "epoch": 2.5773554939466976,
      "grad_norm": 2.8252522945404053,
      "learning_rate": 7.044075100888376e-06,
      "loss": 1.7624,
      "step": 185000
    },
    {
      "epoch": 2.584321319606013,
      "grad_norm": 2.966839075088501,
      "learning_rate": 6.927978006566452e-06,
      "loss": 1.7654,
      "step": 185500
    },
    {
      "epoch": 2.591287145265328,
      "grad_norm": 2.579002857208252,
      "learning_rate": 6.811880912244528e-06,
      "loss": 1.7759,
      "step": 186000
    },
    {
      "epoch": 2.598252970924644,
      "grad_norm": 2.6745476722717285,
      "learning_rate": 6.695783817922606e-06,
      "loss": 1.7627,
      "step": 186500
    },
    {
      "epoch": 2.605218796583959,
      "grad_norm": 1.8105628490447998,
      "learning_rate": 6.579686723600682e-06,
      "loss": 1.7921,
      "step": 187000
    },
    {
      "epoch": 2.6121846222432747,
      "grad_norm": 1.9902032613754272,
      "learning_rate": 6.463589629278758e-06,
      "loss": 1.7651,
      "step": 187500
    },
    {
      "epoch": 2.61915044790259,
      "grad_norm": 2.638460636138916,
      "learning_rate": 6.347492534956835e-06,
      "loss": 1.7762,
      "step": 188000
    },
    {
      "epoch": 2.6261162735619052,
      "grad_norm": 2.5053024291992188,
      "learning_rate": 6.231395440634912e-06,
      "loss": 1.749,
      "step": 188500
    },
    {
      "epoch": 2.6330820992212205,
      "grad_norm": 2.4477484226226807,
      "learning_rate": 6.115298346312988e-06,
      "loss": 1.7537,
      "step": 189000
    },
    {
      "epoch": 2.640047924880536,
      "grad_norm": 2.3978443145751953,
      "learning_rate": 5.9992012519910655e-06,
      "loss": 1.7658,
      "step": 189500
    },
    {
      "epoch": 2.6470137505398514,
      "grad_norm": 35.5915412902832,
      "learning_rate": 5.883104157669142e-06,
      "loss": 1.7553,
      "step": 190000
    },
    {
      "epoch": 2.653979576199167,
      "grad_norm": 2.365534782409668,
      "learning_rate": 5.767007063347219e-06,
      "loss": 1.7572,
      "step": 190500
    },
    {
      "epoch": 2.6609454018584824,
      "grad_norm": 2.512824535369873,
      "learning_rate": 5.650909969025295e-06,
      "loss": 1.781,
      "step": 191000
    },
    {
      "epoch": 2.6679112275177976,
      "grad_norm": 2.3033885955810547,
      "learning_rate": 5.534812874703372e-06,
      "loss": 1.7672,
      "step": 191500
    },
    {
      "epoch": 2.674877053177113,
      "grad_norm": 2.237483501434326,
      "learning_rate": 5.418715780381449e-06,
      "loss": 1.7606,
      "step": 192000
    },
    {
      "epoch": 2.6818428788364286,
      "grad_norm": 2.397895097732544,
      "learning_rate": 5.302618686059525e-06,
      "loss": 1.7771,
      "step": 192500
    },
    {
      "epoch": 2.688808704495744,
      "grad_norm": 1.9772799015045166,
      "learning_rate": 5.1865215917376025e-06,
      "loss": 1.7636,
      "step": 193000
    },
    {
      "epoch": 2.6957745301550595,
      "grad_norm": 2.31284761428833,
      "learning_rate": 5.070424497415679e-06,
      "loss": 1.7597,
      "step": 193500
    },
    {
      "epoch": 2.7027403558143748,
      "grad_norm": 2.0961132049560547,
      "learning_rate": 4.954327403093756e-06,
      "loss": 1.7598,
      "step": 194000
    },
    {
      "epoch": 2.70970618147369,
      "grad_norm": 2.568810224533081,
      "learning_rate": 4.8382303087718325e-06,
      "loss": 1.7769,
      "step": 194500
    },
    {
      "epoch": 2.7166720071330053,
      "grad_norm": 2.583944082260132,
      "learning_rate": 4.722133214449909e-06,
      "loss": 1.7661,
      "step": 195000
    },
    {
      "epoch": 2.723637832792321,
      "grad_norm": 1.9051694869995117,
      "learning_rate": 4.606036120127986e-06,
      "loss": 1.7914,
      "step": 195500
    },
    {
      "epoch": 2.730603658451636,
      "grad_norm": 2.463027000427246,
      "learning_rate": 4.489939025806062e-06,
      "loss": 1.7841,
      "step": 196000
    },
    {
      "epoch": 2.737569484110952,
      "grad_norm": 2.202361583709717,
      "learning_rate": 4.37384193148414e-06,
      "loss": 1.7751,
      "step": 196500
    },
    {
      "epoch": 2.744535309770267,
      "grad_norm": 2.5062572956085205,
      "learning_rate": 4.257744837162216e-06,
      "loss": 1.7448,
      "step": 197000
    },
    {
      "epoch": 2.7515011354295824,
      "grad_norm": 2.0123355388641357,
      "learning_rate": 4.141647742840292e-06,
      "loss": 1.7735,
      "step": 197500
    },
    {
      "epoch": 2.7584669610888977,
      "grad_norm": 1.913692593574524,
      "learning_rate": 4.025550648518369e-06,
      "loss": 1.7719,
      "step": 198000
    },
    {
      "epoch": 2.7654327867482134,
      "grad_norm": 2.151370048522949,
      "learning_rate": 3.909453554196446e-06,
      "loss": 1.7774,
      "step": 198500
    },
    {
      "epoch": 2.7723986124075286,
      "grad_norm": 2.470611333847046,
      "learning_rate": 3.7933564598745227e-06,
      "loss": 1.7717,
      "step": 199000
    },
    {
      "epoch": 2.7793644380668443,
      "grad_norm": 2.95093035697937,
      "learning_rate": 3.677259365552599e-06,
      "loss": 1.7511,
      "step": 199500
    },
    {
      "epoch": 2.7863302637261596,
      "grad_norm": 2.082022190093994,
      "learning_rate": 3.561162271230676e-06,
      "loss": 1.7714,
      "step": 200000
    },
    {
      "epoch": 2.793296089385475,
      "grad_norm": 2.2238376140594482,
      "learning_rate": 3.445065176908752e-06,
      "loss": 1.775,
      "step": 200500
    },
    {
      "epoch": 2.80026191504479,
      "grad_norm": 2.583930015563965,
      "learning_rate": 3.3289680825868294e-06,
      "loss": 1.7539,
      "step": 201000
    },
    {
      "epoch": 2.8072277407041057,
      "grad_norm": 2.7449522018432617,
      "learning_rate": 3.2128709882649057e-06,
      "loss": 1.7628,
      "step": 201500
    },
    {
      "epoch": 2.814193566363421,
      "grad_norm": 2.3249025344848633,
      "learning_rate": 3.0967738939429825e-06,
      "loss": 1.771,
      "step": 202000
    },
    {
      "epoch": 2.8211593920227367,
      "grad_norm": 2.3535807132720947,
      "learning_rate": 2.9806767996210593e-06,
      "loss": 1.7742,
      "step": 202500
    },
    {
      "epoch": 2.828125217682052,
      "grad_norm": 2.7879140377044678,
      "learning_rate": 2.864579705299136e-06,
      "loss": 1.7648,
      "step": 203000
    },
    {
      "epoch": 2.835091043341367,
      "grad_norm": 2.3932607173919678,
      "learning_rate": 2.748482610977213e-06,
      "loss": 1.7688,
      "step": 203500
    },
    {
      "epoch": 2.8420568690006824,
      "grad_norm": 2.694493293762207,
      "learning_rate": 2.6323855166552896e-06,
      "loss": 1.7876,
      "step": 204000
    },
    {
      "epoch": 2.849022694659998,
      "grad_norm": 1.7520289421081543,
      "learning_rate": 2.516288422333366e-06,
      "loss": 1.7789,
      "step": 204500
    },
    {
      "epoch": 2.8559885203193134,
      "grad_norm": 2.1379945278167725,
      "learning_rate": 2.4001913280114424e-06,
      "loss": 1.7626,
      "step": 205000
    },
    {
      "epoch": 2.862954345978629,
      "grad_norm": 2.095000982284546,
      "learning_rate": 2.284094233689519e-06,
      "loss": 1.767,
      "step": 205500
    },
    {
      "epoch": 2.8699201716379443,
      "grad_norm": 2.182884454727173,
      "learning_rate": 2.167997139367596e-06,
      "loss": 1.7762,
      "step": 206000
    },
    {
      "epoch": 2.8768859972972596,
      "grad_norm": 2.4100584983825684,
      "learning_rate": 2.0519000450456727e-06,
      "loss": 1.7902,
      "step": 206500
    },
    {
      "epoch": 2.883851822956575,
      "grad_norm": 2.1290934085845947,
      "learning_rate": 1.9358029507237495e-06,
      "loss": 1.7417,
      "step": 207000
    },
    {
      "epoch": 2.8908176486158905,
      "grad_norm": 2.4516732692718506,
      "learning_rate": 1.8197058564018263e-06,
      "loss": 1.7768,
      "step": 207500
    },
    {
      "epoch": 2.8977834742752058,
      "grad_norm": 2.2054941654205322,
      "learning_rate": 1.7036087620799026e-06,
      "loss": 1.7746,
      "step": 208000
    },
    {
      "epoch": 2.9047492999345215,
      "grad_norm": 2.210495948791504,
      "learning_rate": 1.5875116677579794e-06,
      "loss": 1.748,
      "step": 208500
    },
    {
      "epoch": 2.9117151255938367,
      "grad_norm": 2.158306121826172,
      "learning_rate": 1.471414573436056e-06,
      "loss": 1.7693,
      "step": 209000
    },
    {
      "epoch": 2.918680951253152,
      "grad_norm": 2.429324150085449,
      "learning_rate": 1.3553174791141328e-06,
      "loss": 1.7536,
      "step": 209500
    },
    {
      "epoch": 2.925646776912467,
      "grad_norm": 2.5817203521728516,
      "learning_rate": 1.2392203847922096e-06,
      "loss": 1.7799,
      "step": 210000
    },
    {
      "epoch": 2.932612602571783,
      "grad_norm": 2.7303977012634277,
      "learning_rate": 1.1231232904702863e-06,
      "loss": 1.7744,
      "step": 210500
    },
    {
      "epoch": 2.939578428231098,
      "grad_norm": 2.538878917694092,
      "learning_rate": 1.007026196148363e-06,
      "loss": 1.7746,
      "step": 211000
    },
    {
      "epoch": 2.946544253890414,
      "grad_norm": 2.3410542011260986,
      "learning_rate": 8.909291018264396e-07,
      "loss": 1.7741,
      "step": 211500
    },
    {
      "epoch": 2.953510079549729,
      "grad_norm": 2.5958251953125,
      "learning_rate": 7.748320075045162e-07,
      "loss": 1.7666,
      "step": 212000
    },
    {
      "epoch": 2.9604759052090444,
      "grad_norm": 1.9786360263824463,
      "learning_rate": 6.587349131825929e-07,
      "loss": 1.7656,
      "step": 212500
    },
    {
      "epoch": 2.9674417308683596,
      "grad_norm": 2.6530721187591553,
      "learning_rate": 5.426378188606695e-07,
      "loss": 1.7532,
      "step": 213000
    },
    {
      "epoch": 2.9744075565276753,
      "grad_norm": 3.142993450164795,
      "learning_rate": 4.265407245387463e-07,
      "loss": 1.7595,
      "step": 213500
    },
    {
      "epoch": 2.9813733821869906,
      "grad_norm": 2.742410659790039,
      "learning_rate": 3.104436302168229e-07,
      "loss": 1.7708,
      "step": 214000
    },
    {
      "epoch": 2.9883392078463062,
      "grad_norm": 2.5175256729125977,
      "learning_rate": 1.9434653589489965e-07,
      "loss": 1.7791,
      "step": 214500
    },
    {
      "epoch": 2.9953050335056215,
      "grad_norm": 2.5202980041503906,
      "learning_rate": 7.824944157297631e-08,
      "loss": 1.7423,
      "step": 215000
    },
    {
      "epoch": 3.0,
      "step": 215337,
      "total_flos": 2.2882818626410906e+17,
      "train_loss": 1.8040490566820844,
      "train_runtime": 54713.1806,
      "train_samples_per_second": 15.743,
      "train_steps_per_second": 3.936
    }
  ],
  "logging_steps": 500,
  "max_steps": 215337,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.2882818626410906e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
